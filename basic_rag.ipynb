{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e9416ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=C:\\projects\\ragcode\\.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m296 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m273 packages\u001b[0m \u001b[2min 0.21ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add google-genai python-dotenv numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fabb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyBYq00sNLf-ELfyPFV0iM_D35xsuYC4vXY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc39de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cddafe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.34674243e-03  2.40223552e-03  1.83480582e-03 -1.54992491e-02\n",
      "  7.45987371e-02  5.99153303e-02  3.19676772e-02  2.67468151e-02\n",
      "  2.59960964e-02 -2.96183098e-02  7.61197042e-03  5.58658689e-03\n",
      "  3.26719023e-02  2.53468510e-02  1.35427415e-02  5.78699000e-02\n",
      "  1.17850594e-01  6.70695975e-02 -6.91913627e-03 -4.49889638e-02\n",
      " -5.51183429e-03  2.83483025e-02  9.90352556e-02 -5.59788868e-02\n",
      "  3.67851406e-02 -2.57713255e-03  7.66836200e-03  7.71480054e-02\n",
      "  9.56553444e-02 -8.26762524e-03  5.24235405e-02 -2.45366376e-02\n",
      "  6.39095008e-02  1.72637124e-02  7.01922327e-02  5.68613000e-02\n",
      " -4.50920463e-02  4.26678173e-02 -2.86563262e-02  7.85986185e-02\n",
      "  3.01986728e-02 -3.73339374e-03 -9.89009906e-03  6.54189289e-02\n",
      "  9.82317105e-02 -7.66057819e-02 -5.31081669e-02 -3.27406509e-04\n",
      "  4.13534790e-03  8.49060491e-02 -6.29577413e-02 -3.20573337e-02\n",
      " -7.04800561e-02 -3.50319147e-02 -2.32712403e-02 -4.06736806e-02\n",
      "  1.42755546e-02 -8.09421390e-02  9.88004822e-03 -3.62574644e-02\n",
      "  3.77748758e-02 -4.64594290e-02  5.37510142e-02  1.32146450e-02\n",
      "  1.40138734e-02 -2.74445303e-02 -1.98897440e-02  1.42848551e-01\n",
      " -2.47925464e-02 -3.18149710e-03 -7.24917427e-02  1.55803077e-02\n",
      " -5.47653772e-02  6.78192750e-02 -8.61368254e-02  1.05579384e-02\n",
      " -6.40081540e-02 -1.40917391e-01  6.27024099e-02 -8.31320062e-02\n",
      "  1.46647179e-02 -2.42686365e-02  5.23817018e-02  4.59520742e-02\n",
      "  7.00645242e-03  2.08211094e-02  7.79044181e-02  1.77340712e-02\n",
      " -5.12224995e-02 -1.06098838e-02  4.13188944e-03 -2.46537402e-02\n",
      "  2.81529687e-02  6.65211910e-03 -5.93940280e-02  2.05987487e-02\n",
      " -2.78445296e-02 -4.43434976e-02  2.90866438e-02  8.27917010e-02\n",
      "  9.95282177e-03  6.67690812e-03  8.28091130e-02  2.63493857e-04\n",
      " -9.49992463e-02 -1.13951400e-01  3.10694035e-02 -2.03308631e-02\n",
      " -1.22339968e-02 -4.57900427e-02 -9.61027574e-04 -3.38552259e-02\n",
      " -8.72000083e-02 -5.19399252e-03  4.64219861e-02 -1.16417840e-01\n",
      "  6.04409613e-02 -8.60900432e-02  3.11818928e-03 -5.93203725e-03\n",
      " -6.22655638e-02  4.55832183e-02 -7.16995597e-02 -4.25602449e-03\n",
      " -1.33308331e-02 -1.60188098e-02  6.68109581e-02 -1.11845863e-33\n",
      "  9.83547047e-03  5.06587792e-03  1.50244376e-02  1.10236183e-01\n",
      "  4.08857428e-02 -1.40668163e-02 -5.58365770e-02  3.22771892e-02\n",
      " -8.80328938e-02  1.82437280e-03 -3.96707002e-03 -4.89949770e-02\n",
      "  3.10912561e-02  8.40576515e-02 -3.71657498e-02 -3.23160551e-02\n",
      " -6.20743372e-02 -9.51930508e-03  2.60292124e-02  2.49491655e-03\n",
      " -7.29696602e-02  4.15980034e-02  1.39873391e-02 -3.41003463e-02\n",
      " -6.90056235e-02  2.41250009e-03  2.93620285e-02 -8.08679387e-02\n",
      " -3.58390398e-02 -1.95486657e-02 -5.25196679e-02 -5.84462769e-02\n",
      "  5.72920181e-02 -2.60349531e-02  2.80658416e-02  4.03930712e-03\n",
      "  3.61808166e-02  6.59376569e-03 -2.51837168e-02 -3.33911949e-03\n",
      "  2.34778766e-02 -2.49443259e-02  1.24654889e-01 -5.49946129e-02\n",
      " -2.12579016e-02  1.61116458e-02  6.42328942e-03 -1.73819046e-02\n",
      "  4.08670073e-03 -3.27206962e-02  1.63139528e-04  3.46229114e-02\n",
      " -1.08463923e-02 -3.93749475e-02  9.50971469e-02 -1.08402427e-02\n",
      " -2.82493141e-02  2.48130970e-02  5.61928451e-02  1.89000107e-02\n",
      " -5.36934584e-02 -7.91771151e-03  4.77821082e-02  1.38732325e-02\n",
      "  4.30383645e-02  3.61801428e-03 -3.95822339e-02 -1.11992873e-01\n",
      "  5.27318195e-02  2.66967919e-02 -2.39895787e-02  1.99866202e-02\n",
      " -9.63651389e-02 -3.77832502e-02 -1.06186256e-01  1.35026332e-02\n",
      " -5.11459708e-02 -7.93625694e-03 -1.05659077e-02  4.94473688e-02\n",
      " -3.58213894e-02 -1.49826676e-01  1.42695336e-02 -6.91209361e-02\n",
      " -3.77895012e-02 -6.90801665e-02  4.50409502e-02 -1.66615143e-01\n",
      "  7.55838379e-02 -1.00945476e-02 -5.88838682e-02  1.13463607e-02\n",
      " -8.35671946e-02 -4.67893332e-02  1.02073848e-01 -4.47803999e-35\n",
      "  2.29515247e-02  2.02605687e-02 -9.03231092e-03  5.69148473e-02\n",
      "  5.61938584e-02  8.46668705e-02  2.10885406e-02  4.11245264e-02\n",
      " -1.33801263e-03  1.49054108e-02 -5.84867857e-02 -2.15300992e-02\n",
      "  7.55728930e-02 -7.55578354e-02 -1.32012516e-02  2.11204849e-02\n",
      "  9.30440519e-03  1.96934678e-02  5.60477469e-03  2.65013203e-02\n",
      " -7.00650215e-02 -5.53689292e-03  1.50643671e-02  2.56827362e-02\n",
      "  1.20322905e-01  1.06687829e-01  3.17936838e-02 -7.28855655e-03\n",
      " -2.34720819e-02 -8.55236575e-02 -2.63102613e-02 -4.62083183e-02\n",
      "  1.53831895e-02  3.26381624e-02 -8.25131238e-02 -4.78959503e-03\n",
      "  1.40359700e-01  1.51921529e-02 -3.28214243e-02 -3.67183564e-03\n",
      "  8.15530270e-02  2.44737733e-02 -6.79932684e-02 -6.06219051e-03\n",
      " -3.60504277e-02  1.94107997e-03 -1.14992209e-01 -4.63035814e-02\n",
      "  7.76582882e-02  1.91285613e-03 -6.63784593e-02 -9.44679324e-03\n",
      " -5.02198189e-02 -9.87394340e-03  9.18274652e-03 -8.62740427e-02\n",
      " -3.68765555e-02  1.44498050e-03  3.08640171e-02  1.24916900e-02\n",
      " -3.84530760e-02 -3.87277408e-03 -3.64348060e-03 -4.79012355e-02\n",
      "  5.91625795e-02 -1.26741692e-01 -2.12645717e-02  2.09906623e-02\n",
      " -7.15680793e-02  1.01491371e-02 -1.97857013e-03 -2.35861782e-02\n",
      "  4.42360388e-03  8.49307049e-03  2.66062059e-02  9.02619213e-03\n",
      "  4.21832316e-02  5.75294830e-02 -3.73691432e-02 -7.17524812e-02\n",
      "  1.02208667e-01 -9.84367076e-03  1.06512457e-02  6.20551147e-02\n",
      "  7.82333463e-02  6.12589978e-02 -8.80173780e-03 -4.49705869e-03\n",
      "  1.04322843e-03  3.42901126e-02 -1.25804665e-02  4.55799550e-02\n",
      " -1.04903290e-02  1.26965702e-01  6.01976551e-02 -1.61127520e-08\n",
      " -4.31180485e-02 -1.01540368e-02  1.01238228e-02 -7.86050968e-03\n",
      " -6.33169264e-02 -3.46146827e-03  1.33149768e-03 -9.77645442e-02\n",
      " -3.78194563e-02 -5.11830337e-02  7.59681240e-02 -7.83849210e-02\n",
      " -3.55951115e-02 -3.32821347e-02  4.19391599e-03  4.19164337e-02\n",
      " -2.59904657e-02  7.41204061e-03  3.73922400e-02  3.66032533e-02\n",
      "  1.28250774e-02  1.17808886e-01  1.03884027e-03 -3.47785540e-02\n",
      " -3.39909457e-02  6.01576939e-02 -2.83055026e-02 -2.05665082e-02\n",
      "  2.62764487e-02 -5.09047173e-02  3.15710343e-02  7.06092790e-02\n",
      "  2.36893469e-03 -7.20478222e-02  1.27352029e-02 -1.74590442e-02\n",
      "  2.30147149e-02 -7.31286556e-02 -1.01080779e-02 -3.93641442e-02\n",
      "  1.48912286e-02  3.55265215e-02 -3.99478637e-02 -8.96821395e-02\n",
      "  1.13781057e-01  2.33707801e-02  2.15534084e-02 -5.46623282e-02\n",
      "  2.05254927e-02  1.87117290e-02 -1.38910627e-02 -7.83383325e-02\n",
      "  6.28294144e-03 -7.43739307e-03 -5.96115924e-03  3.57202888e-02\n",
      "  3.20143029e-02  8.60925540e-02 -3.16577926e-02 -2.43858062e-03\n",
      "  1.00977914e-02  8.19234103e-02  2.26312596e-02 -4.83508743e-02]\n"
     ]
    }
   ],
   "source": [
    "a = \"sample text for embedding\"\n",
    "\n",
    "embed = print(model.encode(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8961f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02f605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: C:\\Users\\Shaur\\OneDrive\\Desktop\\ML\\rag_test\\Copy of Minor project- Report prepartion (1).docx.pdf\n",
      "Loading file: C:\\Users\\Shaur\\OneDrive\\Desktop\\ML\\rag_test\\REPORT_NEW (1).pdf\n",
      "\n",
      "--- Summary ---\n",
      "Number of documents (pages) loaded: 32\n",
      "First page content preview: 12 \n",
      " \n",
      "These results demonstrate the utility of multi-temporal, multi-sensor satellite data integration in evaluating ecological change \n",
      "and risks due to anthropogenic activities. The framework is robu...\n",
      "Metadata of the first page: {'producer': 'Skia/PDF m133 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': 'C:\\\\Users\\\\Shaur\\\\OneDrive\\\\Desktop\\\\ML\\\\rag_test\\\\Copy of Minor project- Report prepartion (1).docx.pdf', 'file_path': 'C:\\\\Users\\\\Shaur\\\\OneDrive\\\\Desktop\\\\ML\\\\rag_test\\\\Copy of Minor project- Report prepartion (1).docx.pdf', 'total_pages': 16, 'format': 'PDF 1.4', 'title': 'Copy of Minor project- Report prepartion (1).docx', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = r\"C:\\Users\\Shaur\\OneDrive\\Desktop\\ML\\rag_test\"\n",
    "\n",
    "# Use glob to find all PDF files in the directory\n",
    "pdf_files = glob.glob(os.path.join(directory_path, \"*.pdf\"))\n",
    "\n",
    "# List to hold all documents from all PDFs\n",
    "all_documents = []\n",
    "\n",
    "# Loop through each PDF file and load it\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Loading file: {pdf_file}\")\n",
    "    loader = PyMuPDFLoader(pdf_file)\n",
    "    documents = loader.load()\n",
    "    all_documents.extend(documents)\n",
    "\n",
    "# Print basic information about the loaded documents\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Number of documents (pages) loaded: {len(all_documents)}\")\n",
    "if all_documents:\n",
    "    print(f\"First page content preview: {all_documents[30].page_content[:200]}...\")\n",
    "    print(f\"Metadata of the first page: {all_documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0417cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    # Replace multiple spaces/newlines with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ec8090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432\n",
      "424\n",
      "744\n",
      "162\n",
      "467\n",
      "1\n",
      "3581\n",
      "2562\n",
      "4893\n",
      "429\n",
      "577\n",
      "2525\n",
      "2585\n",
      "1179\n",
      "3634\n",
      "2270\n",
      "457\n",
      "438\n",
      "775\n",
      "137\n",
      "468\n",
      "3309\n",
      "3960\n",
      "2973\n",
      "595\n",
      "767\n",
      "410\n",
      "236\n",
      "494\n",
      "864\n",
      "3590\n",
      "340\n",
      "Total number of cleaned documents stored: 32\n"
     ]
    }
   ],
   "source": [
    "all_cleaned_content = []  # Create an empty list to store all cleaned content\n",
    "\n",
    "for doc in all_documents:\n",
    "    cleaned = clean_pdf_text(doc.page_content)\n",
    "    all_cleaned_content.append(cleaned)\n",
    "    print(len(cleaned))\n",
    "\n",
    "print(f\"Total number of cleaned documents stored: {len(all_cleaned_content)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0a2e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744\n",
      "Declaration\n",
      "I declare that this written submission represents my ideas in my own\n",
      "words and where others' ideas or words have been included, I have\n",
      "adequately cited and referenced the original sources. I also declare that I\n",
      "have adhered to all principles of academic honesty and integrity and have\n",
      "not misrepresented, fabricated, falsified any idea/data/fact/source in my\n",
      "submission. I understand that any violation of the above will be cause for\n",
      "disciplinary action by the Institute and can also evoke penal action from\n",
      "the sources that have thus not been properly cited or from whom proper\n",
      "permission has not been taken when needed.\n",
      "(Signature of Author)\n",
      "_________________\n",
      "Dr. Kavita Jasiwal\n",
      "Assistant Professor\n",
      "Department of CSE\n",
      "(Roll Number)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(all_documents[2].page_content)}\")\n",
    "print(all_documents[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9f1d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 65\n",
      "First chunk:\n",
      "SENTIMENT ANALYSIS ON MOBILE PHONE REVIEWS Project report submitted for IIIrd Semester Minor Project-I in Department of DSAI By, Gaurav Sahu 231020422 Shaurya Kumar 231020446 Sumit Barik 230120455 Department of DSAI Dr. Shyama Prasad Mukherjee International Institute of Information Technology, Naya Raipur (A Joint Initiative of Govt. of Chhattisgarh and NTPC) Email: iiitnr@iiitnr.ac.in, Tel: (0771) 2474040, Web: www.iiitnr.ac.in...\n",
      "Metadata of first chunk: {}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,    # adjust based on your use case\n",
    "    chunk_overlap=100,  # overlap helps preserve context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split the cleaned text\n",
    "chunks = splitter.create_documents(all_cleaned_content)\n",
    "\n",
    "# Inspect results\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"First chunk:\\n{chunks[0].page_content[:500]}...\")\n",
    "print(\"Metadata of first chunk:\", chunks[0].metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d510e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 66\n",
      "First chunk metadata:\n",
      "{'doc_no': 1, 'doc_page_no': 0}\n",
      "Last chunk metadata:\n",
      "{'doc_no': 32, 'doc_page_no': 15}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Initialize splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# A new list to store all chunks with correct metadata\n",
    "all_final_chunks = []\n",
    "doc_no = 0\n",
    "\n",
    "# Loop through the original documents to get page-specific information\n",
    "for doc_index, original_doc in enumerate(all_documents):\n",
    "    # original_doc.page_content contains the text of one page\n",
    "    # original_doc.metadata['source'] and original_doc.metadata['page'] have the original info\n",
    "\n",
    "    # Create a dummy list to hold the single page content for the splitter\n",
    "    page_content_list = [original_doc.page_content]\n",
    "\n",
    "    # Split the content of the current page\n",
    "    chunks = splitter.create_documents(page_content_list)\n",
    "\n",
    "    # Now, iterate through the chunks from this specific page and add metadata\n",
    "    for chunk_index, chunk in enumerate(chunks):\n",
    "        # Create a new Document object with combined metadata\n",
    "        new_metadata = {\n",
    "            \"doc_no\": doc_index + 1,  # Document number (assuming all_documents is a list of documents)\n",
    "            \"doc_page_no\": original_doc.metadata.get('page'),\n",
    "        }\n",
    "        \n",
    "        new_chunk = Document(page_content=chunk.page_content, metadata=new_metadata)\n",
    "        all_final_chunks.append(new_chunk)\n",
    "\n",
    "# Inspect results\n",
    "print(f\"Total chunks: {len(all_final_chunks)}\")\n",
    "print(f\"First chunk metadata:\\n{all_final_chunks[0].metadata}\")\n",
    "print(f\"Last chunk metadata:\\n{all_final_chunks[-1].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61138539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings created: 66\n",
      "Shape of the first embedding: (384,)\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'all_final_chunks' is the list of Document objects with cleaned text and metadata\n",
    "\n",
    "# Create a list of just the text content from the chunks\n",
    "texts_to_embed = [doc.page_content for doc in all_final_chunks]\n",
    "\n",
    "# Now, use your embed_texts function to get embeddings for all the texts\n",
    "all_embeddings = model.encode(texts_to_embed)\n",
    "\n",
    "# Print the number of embeddings created to verify\n",
    "print(f\"Number of embeddings created: {len(all_embeddings)}\")\n",
    "print(f\"Shape of the first embedding: {all_embeddings[0].shape if hasattr(all_embeddings[0], 'shape') else len(all_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbbd20fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 66 vectors\n",
      "Persisted to: C:\\Users\\Shaur\\OneDrive\\Desktop\\ML\\RAG\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Wrap the SentenceTransformer model for LangChain\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a ChromaDB vector store\n",
    "persist_directory = r\"C:\\Users\\Shaur\\OneDrive\\Desktop\\ML\\RAG\\chroma_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_final_chunks,  # Use the list with correct metadata\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"rag_collection\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {vectorstore._collection.count()} vectors\")\n",
    "print(f\"Persisted to: {persist_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6181c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'C:\\\\Users\\\\Shaur\\\\OneDrive\\\\Desktop\\\\ML\\\\rag_test\\\\REPORT_NEW (1).pdf', 'doc_no': 25, 'doc_page_no': 8}, page_content='6 \\n \\nReclassification Ranges: \\n            NDVI VALUE RANGE \\n            VEGETATION CLASS \\n <0.0 \\nNo Vegetation  \\n0.0-0.2 \\nVery Sparse vegetation \\n0.2-0.4 \\nSparse Vegetation \\n0.4-0.6 \\nModerate Vegetation \\n>0.6 \\nDense Forest \\n \\n \\nFigure- 1  NDVI Map \\nImage Interpretation: Dark Green- Dense Forest ,  Light Green- Sparse Forest, White- Barren Land. \\nB. Urbanization Assessment Using NDBI (Landsat-5) \\nObjective: Detect urban growth and impervious surfaces through the Normalized Difference Built-up Index. \\nMethodology: \\n●\\u200b\\nDataset: LANDSAT/LT05/C02/T1_L2 (2005) \\n●\\u200b\\nBands Used: SR_B5 (SWIR1), SR_B4 (NIR) \\n●\\u200b\\nFormula: (SWIR1-NIR) / (SWIR1+NIR) \\nInterpretation:'),\n",
       "  0.6190654039382935),\n",
       " (Document(metadata={'doc_no': 25, 'doc_page_no': 8}, page_content='6 \\n \\nReclassification Ranges: \\n            NDVI VALUE RANGE \\n            VEGETATION CLASS \\n <0.0 \\nNo Vegetation  \\n0.0-0.2 \\nVery Sparse vegetation \\n0.2-0.4 \\nSparse Vegetation \\n0.4-0.6 \\nModerate Vegetation \\n>0.6 \\nDense Forest \\n \\n \\nFigure- 1  NDVI Map \\nImage Interpretation: Dark Green- Dense Forest ,  Light Green- Sparse Forest, White- Barren Land. \\nB. Urbanization Assessment Using NDBI (Landsat-5) \\nObjective: Detect urban growth and impervious surfaces through the Normalized Difference Built-up Index. \\nMethodology: \\n●\\u200b\\nDataset: LANDSAT/LT05/C02/T1_L2 (2005) \\n●\\u200b\\nBands Used: SR_B5 (SWIR1), SR_B4 (NIR) \\n●\\u200b\\nFormula: (SWIR1-NIR) / (SWIR1+NIR) \\nInterpretation:'),\n",
       "  0.6190654039382935),\n",
       " (Document(metadata={'doc_page_no': 9, 'doc_no': 26, 'source': 'C:\\\\Users\\\\Shaur\\\\OneDrive\\\\Desktop\\\\ML\\\\rag_test\\\\REPORT_NEW (1).pdf'}, page_content='7 \\n \\n●\\u200b\\nHigher NDBI values signify urbanized or built-up areas. \\n●\\u200b\\nSpatial pattern analysis shows the expansion of concrete infrastructure. \\n●\\u200b\\nReclassification Ranges: \\nNDBI Value Range \\nLand Surface Type \\n<0.1 \\nWater or Dense Vegetation \\n-0.1 to 0.0 \\nLow Vegetation / Bare soil \\n0.0 to 0.2 \\nTransitional/ Semi Urban \\n>0.2 \\nUrban/ Built Up Area \\nTools: GEE, Landsat-5 imagery \\n \\nFigure- 2  NDBI Map \\nImage Interpretation: Blue and Green- forest , Red and yellow- Concrete or  Barren Land. \\nC. Forest Fire Risk Estimation Using NDVI and Soil Moisture \\nObjective: Identify fire-prone zones based on vegetation and moisture content. \\nMethodology: \\n●\\u200b\\nDatasets: MODIS/006/MOD13Q1 (NDVI) and ECMWF/ERA5_LAND/HOURLY (Soil Moisture) \\n●\\u200b\\nTime Range: Dec 2022 to Jan 2023 (NDVI), Jan 2020 to May 2020 (Soil Moisture)'),\n",
       "  0.6324175596237183)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is nvdi\"\n",
    "results_scores=vectorstore.similarity_search_with_score(query,k=3)\n",
    "results_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f27bbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Gemma, an open-weights AI assistant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# Initialize the Groq model\n",
    "# The ChatGroq class will automatically look for the GROQ_API_KEY environment variable.\n",
    "llm = ChatGroq(\n",
    "    model=\"gemma2-9b-it\"\n",
    ")\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(\"Hi, what model are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83952237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7403b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001D9DDD5A540>, search_kwargs={})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert vector store to retriever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_kwarg={\"k\":3} ## Retrieve top 3 relevant chunks\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2f682c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt=\"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context: {context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82f21992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\\nContext: {context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001D9F2E7FB00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D9F2E7F860>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create a document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ddde37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001D9DDD5A540>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\n\\nContext: {context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001D9F2E7FB00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D9F2E7F860>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create The Final RAG Chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "rag_chain=create_retrieval_chain(retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3bb7971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=rag_chain.invoke({\"input\":\"who are authors of the report\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39935201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'what is nvdi', 'context': [Document(metadata={'doc_no': 25, 'doc_page_no': 8}, page_content='6 \\n \\nReclassification Ranges: \\n            NDVI VALUE RANGE \\n            VEGETATION CLASS \\n <0.0 \\nNo Vegetation  \\n0.0-0.2 \\nVery Sparse vegetation \\n0.2-0.4 \\nSparse Vegetation \\n0.4-0.6 \\nModerate Vegetation \\n>0.6 \\nDense Forest \\n \\n \\nFigure- 1  NDVI Map \\nImage Interpretation: Dark Green- Dense Forest ,  Light Green- Sparse Forest, White- Barren Land. \\nB. Urbanization Assessment Using NDBI (Landsat-5) \\nObjective: Detect urban growth and impervious surfaces through the Normalized Difference Built-up Index. \\nMethodology: \\n●\\u200b\\nDataset: LANDSAT/LT05/C02/T1_L2 (2005) \\n●\\u200b\\nBands Used: SR_B5 (SWIR1), SR_B4 (NIR) \\n●\\u200b\\nFormula: (SWIR1-NIR) / (SWIR1+NIR) \\nInterpretation:'), Document(metadata={'doc_page_no': 10, 'doc_no': 27}, page_content='8 \\n \\n●\\u200b\\nFormula:  NDVI norm= (NDVI-0.1) / (0.8-0.1) * Fire_risk =  NDVI norm * (1-soil_mositure) \\nOutput: Fire risk index map ranging from 0 (low) to 1 (high) \\nTools: GEE, MODIS, ERA5-Land data \\n \\nFigure- 3  Fire Risk \\nImage Interpretation: Blue and Green- Less fire prone area , Red and Yellow - High fire prone area \\n \\nD. Urban Heat Island (UHI) Analysis Using MODIS LST \\nObjective: Detect temperature anomalies in urban areas.'), Document(metadata={'doc_no': 26, 'doc_page_no': 9}, page_content='7 \\n \\n●\\u200b\\nHigher NDBI values signify urbanized or built-up areas. \\n●\\u200b\\nSpatial pattern analysis shows the expansion of concrete infrastructure. \\n●\\u200b\\nReclassification Ranges: \\nNDBI Value Range \\nLand Surface Type \\n<0.1 \\nWater or Dense Vegetation \\n-0.1 to 0.0 \\nLow Vegetation / Bare soil \\n0.0 to 0.2 \\nTransitional/ Semi Urban \\n>0.2 \\nUrban/ Built Up Area \\nTools: GEE, Landsat-5 imagery \\n \\nFigure- 2  NDBI Map \\nImage Interpretation: Blue and Green- forest , Red and yellow- Concrete or  Barren Land. \\nC. Forest Fire Risk Estimation Using NDVI and Soil Moisture \\nObjective: Identify fire-prone zones based on vegetation and moisture content. \\nMethodology: \\n●\\u200b\\nDatasets: MODIS/006/MOD13Q1 (NDVI) and ECMWF/ERA5_LAND/HOURLY (Soil Moisture) \\n●\\u200b\\nTime Range: Dec 2022 to Jan 2023 (NDVI), Jan 2020 to May 2020 (Soil Moisture)'), Document(metadata={'doc_no': 23, 'doc_page_no': 6}, page_content='The project utilized open-access satellite datasets available on Google Earth Engine, such as: \\n●\\u200b\\nSentinel-2 SR: For NDVI-based forest classification. \\n●\\u200b\\nLandsat-5 SR: For NDBI and urban land cover change. \\n●\\u200b\\nMODIS MOD13Q1: NDVI for vegetation health and fire risk. \\n●\\u200b\\nMODIS MOD11A2: Land Surface Temperature (LST) for UHI analysis. \\n●\\u200b\\nERA5-Land: Soil moisture for fire risk computation. \\n●\\u200b\\nGLDAS CLSM: Groundwater storage (GWS_tavg) for long-term water monitoring.')], 'answer': 'NDVI stands for Normalized Difference Vegetation Index.  It is a measure of vegetation health and density derived from satellite imagery.  NDVI values range from -1 to 1, with higher values indicating greater vegetation abundance. \\n'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4017011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The plagiarism of the minor project report must be less than 10% for final acceptance. \\n\\nThe authors of the report are Gaurav Sahu, Shaurya Kumar, and Sumit Barik.  \\nThe date is not provided in the context. \\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragcode (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
