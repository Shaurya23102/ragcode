{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9814ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4894fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up Completed!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import(\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "print(\"Set up Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc611027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure\n",
      "Content :This is the main text content that will be embedded and searched.\n",
      "Metadata :{'source': 'example.txt', 'page': 1, 'author': 'Krish Naik', 'date_created': '2024-01-01', 'cutom_field': 'any_value'}\n",
      "\n",
      "üìù Metadata is crucial for:\n",
      "- Filtering search results\n",
      "- Tracking document sources\n",
      "- Providing context in responses\n",
      "- Debugging and auditing\n"
     ]
    }
   ],
   "source": [
    "## create a simple document\n",
    "doc=Document(\n",
    "    page_content=\"This is the main text content that will be embedded and searched.\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"page\":1,\n",
    "        \"author\":\"Krish Naik\",\n",
    "        \"date_created\":\"2024-01-01\",\n",
    "        \"cutom_field\":\"any_value\"\n",
    "\n",
    "    }\n",
    ")\n",
    "print(\"Document Structure\")\n",
    "\n",
    "print(f\"Content :{doc.page_content}\")\n",
    "print(f\"Metadata :{doc.metadata}\")\n",
    "\n",
    "# Why metadata matters:\n",
    "print(\"\\nüìù Metadata is crucial for:\")\n",
    "print(\"- Filtering search results\")\n",
    "print(\"- Tracking document sources\")\n",
    "print(\"- Providing context in responses\")\n",
    "print(\"- Debugging and auditing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8bd18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "page_content=\"This is the main text content that will be embedded and searched.\"\n",
    "print(len(page_content))\n",
    "words = page_content.split()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee79ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1381072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"‚úÖ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6774b73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DirectoryLoader\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m## load all the text files from the directory\u001b[39;00m\n\u001b[32m      4\u001b[39m dir_loader=DirectoryLoader(\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdata/text_files\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     glob=\u001b[33m\"\u001b[39m\u001b[33m**/*.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m## Pattern to match files  \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     loader_cls= \u001b[43mTextLoader\u001b[49m, \u001b[38;5;66;03m##loader class to use\u001b[39;00m\n\u001b[32m      8\u001b[39m     loader_kwargs={\u001b[33m'\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m},\n\u001b[32m      9\u001b[39m     show_progress=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m documents=dir_loader.load()\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÅ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'TextLoader' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=True\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "\n",
    "print(f\"üìÅ Loaded {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Length: {len(doc.page_content)} characters\")\n",
    "\n",
    "\n",
    "# üìä Analysis\n",
    "print(\"\\nüìä DirectoryLoader Characteristics:\")\n",
    "print(\"‚úÖ Advantages:\")\n",
    "print(\"  - Loads multiple files at once\")\n",
    "print(\"  - Supports glob patterns\")\n",
    "print(\"  - Progress tracking\")\n",
    "print(\"  - Recursive directory scanning\")\n",
    "\n",
    "print(\"\\n‚ùå Disadvantages:\")\n",
    "print(\"  - All files must be same type\")\n",
    "print(\"  - Limited error handling per file\")\n",
    "print(\"  - Can be memory intensive for large directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5552b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 1 document\n",
      "Content preview: Python Programming Introduction\n",
      "\n",
      "Python is a high-level, interpreted programming language known for ...\n",
      "Metadata: {'source': 'data/text_files/python_intro.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "## Loading a single text file\n",
    "loader=TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "documents=loader.load()\n",
    "print(f\"üìÑ Loaded {len(documents)} document\")\n",
    "print(f\"Content preview: {documents[0].page_content[:100]}...\")\n",
    "print(f\"Metadata: {documents[0].metadata}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58c6d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'C:\\\\Users\\\\Shaur\\\\OneDrive\\\\Desktop\\\\ML\\\\test.txt'}, page_content='this is going to be my first test file for loading a text file to vs code and applying parsing techniques to it\\nafter this we will apply parsing tecchniques \\nand then store it in a vector database\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Create an instance of TextLoader with the file path and encoding\n",
    "loader = TextLoader(\"C:\\\\Users\\\\Shaur\\\\OneDrive\\\\Desktop\\\\ML\\\\test.txt\", encoding=\"utf-8\")\n",
    "\n",
    "# Load the documents\n",
    "documents = loader.load()\n",
    "\n",
    "# Print the loaded documents to verifyuv \n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f37b5fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is going to be my first test file for loading a text file to vs code and applying parsing techniques to it\n",
      "after this we will apply parsing tecchniques \n",
      "and then store it in a vector database\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc11374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is going to be my first test file for loading a text file to vs code and applying parsing techniques to it\n",
      "after this we will apply parsing tecchniques \n",
      "and then store it in a vector database\n",
      "\n",
      "197\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'documents' is the list loaded by TextLoader\n",
    "first_document = documents[0]\n",
    "\n",
    "# Now, access the page_content of that specific document\n",
    "page = first_document.page_content\n",
    "print(page)\n",
    "print(len(page))\n",
    "a = page.split()\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad1c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ CHARACTER TEXT SPLITTER\n",
      "Created 5 chunks\n",
      "First chunk: this is going to be my first test file for loading a text...\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# Access the page_content of the first document in the list\n",
    "text_to_split = documents[0].page_content\n",
    "\n",
    "print(\"1Ô∏è‚É£ CHARACTER TEXT SPLITTER\")\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=60,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Pass the string content (text_to_split) to the splitter\n",
    "char_chunks = char_splitter.split_text(text_to_split)\n",
    "\n",
    "print(f\"Created {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk: {char_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b60bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: this is going to be my first test file for loading a text\n",
      "Chunk 2: for loading a text file to vs code and applying parsing\n",
      "Chunk 3: and applying parsing techniques to it\n",
      "after this we will\n",
      "Chunk 4: this we will apply parsing tecchniques \n",
      "and then store it in\n",
      "Chunk 5: then store it in a vector database\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(char_chunks):\n",
    "    print(f\"Chunk {i + 1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f681c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is going to be my first test file for loading a text\n"
     ]
    }
   ],
   "source": [
    "print(char_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cf8fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'C:\\\\Users\\\\Shaur\\\\OneDrive\\\\Desktop\\\\ML\\\\test.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f3be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 111, which is longer than the specified 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ CHARACTER TEXT SPLITTER\n",
      "Created 3 chunks\n",
      "First chunk: this is going to be my first test file for loading a text file to vs code and applying parsing techn...\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Character-based splitting\n",
    "print(\"1Ô∏è‚É£ CHARACTER TEXT SPLITTER\")\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",  # Split on newlines\n",
    "    chunk_size=50,  # Max chunk size in characters\n",
    "    chunk_overlap=20,  # Overlap between chunks\n",
    "    length_function=len  # How to measure chunk size\n",
    ")\n",
    "\n",
    "char_chunks=char_splitter.split_text(text_to_split)\n",
    "print(f\"Created {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk: {char_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb164482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ RECURSIVE CHARACTER TEXT SPLITTER\n",
      "Created 6 chunks\n",
      "First chunk: this is going to be my first test file for loading...\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Recursive character splitting (RECOMMENDED)\n",
    "print(\"\\n2Ô∏è‚É£ RECURSIVE CHARACTER TEXT SPLITTER\")\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],  # Try these separators in order\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(text_to_split)\n",
    "print(f\"Created {len(recursive_chunks)} chunks\")\n",
    "print(f\"First chunk: {recursive_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb75d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple text example - 4 chunks:\n",
      "\n",
      "Chunk 1: 'This is sentence one and it is quite long. This is sentence two and it is also'\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "Chunk 4: 'is sentence five. This is sentence six.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create text without natural break points\n",
    "simple_text = \"This is sentence one and it is quite long. This is sentence two and it is also quite long. This is sentence three which is even longer than the others. This is sentence four. This is sentence five. This is sentence six.\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],  # Only split on spaces\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(simple_text)\n",
    "\n",
    "print(f\"\\nSimple text example - {len(chunks)} chunks:\\n\")\n",
    "\n",
    "for i in range(len(chunks) - 1):\n",
    "    print(f\"Chunk {i+1}: '{chunks[i]}'\")\n",
    "    print(f\"Chunk {i+2}: '{chunks[i+1]}'\")\n",
    "    \n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5ee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ TOKEN TEXT SPLITTER\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TokenTextSplitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Method 3: Token-based splitting\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m3Ô∏è‚É£ TOKEN TEXT SPLITTER\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m token_splitter = \u001b[43mTokenTextSplitter\u001b[49m(\n\u001b[32m      4\u001b[39m     chunk_size=\u001b[32m50\u001b[39m,  \u001b[38;5;66;03m# Size in tokens (not characters)\u001b[39;00m\n\u001b[32m      5\u001b[39m     chunk_overlap=\u001b[32m10\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m token_chunks = token_splitter.split_text(text_to_split)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(token_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'TokenTextSplitter' is not defined"
     ]
    }
   ],
   "source": [
    "# Method 3: Token-based splitting\n",
    "print(\"\\n3Ô∏è‚É£ TOKEN TEXT SPLITTER\")\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,  # Size in tokens (not characters)\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(text_to_split)\n",
    "print(f\"Created {len(token_chunks)} chunks\")\n",
    "print(f\"First chunk: {token_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660f944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KO\n"
     ]
    }
   ],
   "source": [
    "print(\"KO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d52c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4060d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b302b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragcode (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
